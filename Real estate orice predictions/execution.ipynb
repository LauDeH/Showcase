{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.metrics import  mean_squared_error, median_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import shap\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.covariance import MinCovDet\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import shap\n",
    "\n",
    "# WE IMPORT OUR OWN LIBRARY DEFINITIONS \n",
    "import finale as load\n",
    "\n",
    "\n",
    "# Load the CSV file\n",
    "data = pd.read_csv(\"XXX"\n",
    "\n",
    "# Create a subset with only houses\n",
    "housesdata = data[data[\"housingType\"] == \"HOUSE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housesdata_cleaned = load.feature_engineering(housesdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = housesdata_cleaned[(housesdata_cleaned['firstListing'] < 2024)]\n",
    "test_data = housesdata_cleaned[housesdata_cleaned['firstListing'] == 2024]\n",
    "\n",
    "\n",
    "train_median_mode = load.impute_missing_values_with_median_mode(train_data)\n",
    "test_median_mode = load.impute_missing_values_with_median_mode(test_data)\n",
    "\n",
    "\n",
    "\n",
    "train_with_poly = load.generate_polynomial_features(train_median_mode)\n",
    "test_with_poly = load.generate_polynomial_features(test_median_mode)\n",
    "\n",
    "\n",
    "train_data = load.detect_and_remove_outliers(train_with_poly)\n",
    "test_data = test_with_poly\n",
    "\n",
    "param_dist = {\n",
    "    'alpha': [0.01, 0.05, 0.1, 0.5, 1.0, 10.0, 100.0],\n",
    "    'l1_ratio': [0, 0.1, 0.5, 0.7, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "elastic_net, mae, mse, y_pred , X_train, y_train, X_test, y_test = load.perform_elastic_net(train_data, test_data, param_dist)\n",
    "elastic_net_reduced, mae_reduced, mse_reduced, y_pred_reduced = load.perform_elastic_net_with_feature_selection(elastic_net, X_train, y_train, X_test, y_test, param_dist)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],\n",
    "    'min_child_weight': [1, 3, 5, 7]\n",
    "}\n",
    "\n",
    "final_xgb, mae_test, mse_test, shap_values = load.train_xgboost_model(X_train, X_test, y_train, y_test, param_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MICE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = housesdata_cleaned[(housesdata_cleaned['firstListing'] < 2024)]\n",
    "test_data = housesdata_cleaned[housesdata_cleaned['firstListing'] == 2024]\n",
    "\n",
    "\n",
    "train_imputed = load.preprocess_data(train_data)\n",
    "test_imputed = load.preprocess_data(test_data)\n",
    "\n",
    "train_with_poly = load.generate_polynomial_features(train_imputed)\n",
    "test_with_poly = load.generate_polynomial_features(test_imputed)\n",
    "\n",
    "\n",
    "train_data = load.detect_and_remove_outliers(train_with_poly)\n",
    "test_data = test_with_poly\n",
    "\n",
    "param_dist = {\n",
    "    'alpha': [0.01, 0.05, 0.1, 0.5, 1.0, 10.0, 100.0],\n",
    "    'l1_ratio': [0, 0.1, 0.5, 0.7, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "elastic_net, mae, mse, y_pred , X_train, y_train, X_test, y_test = load.perform_elastic_net(train_data, test_data, param_dist)\n",
    "elastic_net_reduced, mae_reduced, mse_reduced, y_pred_reduced = load.perform_elastic_net_with_feature_selection(elastic_net, X_train, y_train, X_test, y_test, param_dist)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],\n",
    "    'min_child_weight': [1, 3, 5, 7]\n",
    "}\n",
    "\n",
    "final_xgb, mae_test, mse_test, shap_values = load.train_xgboost_model(X_train, X_test, y_train, y_test, param_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = housesdata_cleaned[(housesdata_cleaned['firstListing'] < 2024)]\n",
    "test_data = housesdata_cleaned[housesdata_cleaned['firstListing'] == 2024]\n",
    "\n",
    "\n",
    "train_imputed = load.preprocess_data(train_data)\n",
    "test_imputed = load.preprocess_data(test_data)\n",
    "\n",
    "train_with_poly = load.generate_polynomial_features(train_imputed)\n",
    "test_with_poly = load.generate_polynomial_features(test_imputed)\n",
    "\n",
    "\n",
    "train_data = load.detect_and_remove_outliers(train_with_poly)\n",
    "test_data = test_with_poly\n",
    "\n",
    "param_dist = {\n",
    "    'alpha': [0.01, 0.025, 0.05, 0.1, 0.5, 1.0, 10.0, 100.0],\n",
    "    'l1_ratio': [0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "elastic_net, mae, mse, y_pred , X_train, y_train, X_test, y_test = load.perform_elastic_net(train_data, test_data, param_dist)\n",
    "elastic_net_reduced, mae_reduced, mse_reduced, y_pred_reduced = load.perform_elastic_net_with_feature_selection(elastic_net, X_train, y_train, X_test, y_test, param_dist)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300, 500, 1000],\n",
    "    'max_depth': [3, 5, 7, 9, 11],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.15, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],\n",
    "    'min_child_weight': [1, 3, 5, 7]\n",
    "}\n",
    "\n",
    "final_xgb, mae_test, mse_test, shap_values = load.train_xgboost_model(X_train, X_test, y_train, y_test, param_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train data last 4 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = housesdata_cleaned[(housesdata_cleaned['firstListing'] < 2024)]\n",
    "test_data = housesdata_cleaned[housesdata_cleaned['firstListing'] == 2024]\n",
    "\n",
    "\n",
    "train_imputed = load.preprocess_data(train_data)\n",
    "test_imputed = load.preprocess_data(test_data)\n",
    "\n",
    "train_with_poly = load.generate_polynomial_features(train_imputed)\n",
    "test_with_poly = load.generate_polynomial_features(test_imputed)\n",
    "\n",
    "train_data = train_with_poly[(train_with_poly['firstListing'] >= 2020) & (train_with_poly['firstListing'] < 2024)]\n",
    "test_data = test_with_poly[test_with_poly['firstListing'] == 2024]\n",
    "\n",
    "\n",
    "train_data = load.detect_and_remove_outliers(train_data)\n",
    "test_data = test_data\n",
    "\n",
    "param_dist = {\n",
    "    'alpha': [0.01, 0.05, 0.1, 0.5, 1.0, 10.0, 100.0],\n",
    "    'l1_ratio': [0, 0.1, 0.5, 0.7, 0.9, 1.0]\n",
    "}\n",
    "elastic_net, mae, mse, y_pred , X_train, y_train, X_test, y_test = load.perform_elastic_net(train_data, test_data, param_dist)\n",
    "elastic_net_reduced, mae_reduced, mse_reduced, y_pred_reduced = load.perform_elastic_net_with_feature_selection(elastic_net, X_train, y_train, X_test, y_test, param_dist)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 9],\n",
    "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2, 0.3],\n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2, 0.5],\n",
    "    'min_child_weight': [1, 3, 5, 7]\n",
    "}\n",
    "\n",
    "final_xgb, mae_test, mse_test, shap_values = load.train_xgboost_model(X_train, X_test, y_train, y_test, param_dist)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
